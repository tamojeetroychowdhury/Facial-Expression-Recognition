{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamojeetroychowdhury/Facial-Expression-Recognition/blob/main/Week-2/FER_DeX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcboyZk38nQL",
        "outputId": "5d271b88-d4d9-4c1c-dddb-afdc7025041e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W7w1RTjh8zv9"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vVRdG1O_-5j7"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pMJ87kLG-tPn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru4FYfGf9_O2",
        "outputId": "a9b427a4-8582-475f-f4ef-5d66846496aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4],\n",
              "       [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "np.array([1,2,3,4,5,6]).reshape(3,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YInIpbd2-YRW"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,84):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/surprise/SU ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(0)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,70):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/happiness/H ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(1)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,26):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/fear/F ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(2)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,60):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/disgust/D ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(3)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,594):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/neutral/N ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(4)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,29):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/sadness/S ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(5)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "for i in range(1,46):\n",
        "  img = Image.open(f'gdrive/My Drive/FER/CK+/anger/A ({i}).png').convert('L')\n",
        "  try:\n",
        "    data = np.array(img.getdata()).reshape((490,640))\n",
        "    images.append([data])\n",
        "    labels.append(6)\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI1g5Kwb_eXu",
        "outputId": "6b2dca3c-7a58-43cf-a7e3-8f112179b7f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVdtje94DDEL",
        "outputId": "87a4a87a-f441-4678-faf3-3df6f601adec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(845, 1, 490, 640)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "np.array(images).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D-8LL3d2DF9N"
      },
      "outputs": [],
      "source": [
        "shuffler = [[images[i], labels[i]] for i in range(845)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CpIkgdVrDWdl"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(shuffler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4-sC_ixzDed8"
      },
      "outputs": [],
      "source": [
        "images_new = [shuffler[i][0] for i in range(845)]\n",
        "labels_new = [shuffler[i][1] for i in range(845)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY023Md9D19x",
        "outputId": "cf0ab8bb-0afd-4e79-dad5-eb9304749a7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(845,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "np.array(labels_new).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tDLdTwNeD4GI"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#df = pd.read_csv('gdrive/My Drive/fer2013.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "beEVuJwKEEYU"
      },
      "outputs": [],
      "source": [
        "images_new = np.array(images_new, dtype = np.float32)\n",
        "dataset = torch.tensor(images_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dXOnsH3EKnQ",
        "outputId": "822b5c6f-d507-4346-c8ff-04cb88e98302"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([845, 1, 490, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od826DwNFFIC"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3-zdTR60EX9N"
      },
      "outputs": [],
      "source": [
        "input_size = 2304\n",
        "hidden_size = 500 \n",
        "num_classes = 10\n",
        "#num_epochs = 3\n",
        "#batch_size = 2000\n",
        "learning_rate = 0.01\n",
        "\n",
        "b = 20\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 7, stride = 2, padding = 3)\n",
        "        self.pool1 = nn.MaxPool2d(3, 2)\n",
        "        self.lrn1 = nn.LocalResponseNorm(64)\n",
        "\n",
        "        self.conv2a = nn.Conv2d(64, 96, 1)\n",
        "        self.conv2b = nn.Conv2d(96, 208, 3, padding = 1)\n",
        "        self.pool2a = nn.MaxPool2d(3, 1, padding = 1)\n",
        "        self.conv2c = nn.Conv2d(64, 64, 1)\n",
        "        #self.cat2 = torch.cat()\n",
        "        self.pool2b = nn.MaxPool2d(3, 2)\n",
        "\n",
        "        self.conv3a = nn.Conv2d(272, 96, 1)\n",
        "        self.conv3c = nn.Conv2d(272, 64, 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(6152640//b, 90)\n",
        "        self.fc3 = nn.Linear(90, 7)\n",
        "\n",
        "        #self.lkr = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrn1(self.pool1(F.relu(self.conv1(x))))\n",
        "\n",
        "        y = F.relu(self.conv2c(self.pool2a(x)))\n",
        "        z = F.relu(self.conv2b(F.relu(self.conv2a(x))))\n",
        "        x = self.pool2b(torch.cat((y,z),1))\n",
        "\n",
        "        y = F.relu(self.conv3c(self.pool2a(x)))\n",
        "        z = F.relu(self.conv2b(F.relu(self.conv3a(x))))\n",
        "        x = self.pool2b(torch.cat((y,z),1))\n",
        "\n",
        "        x = x.view(-1, 6152640//b)\n",
        "        x = F.relu(self.fc3(F.relu(self.fc1(x))))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = NeuralNet()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "au09wdRDFWI9"
      },
      "outputs": [],
      "source": [
        "labels_new = torch.tensor(labels_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9kSqVYD5FGRL",
        "outputId": "439ed2a0-7270-4811-b10c-ae919351aaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [1], Loss: 1.8590\n",
            "Epoch [1/4], Step [2], Loss: 160475.4688\n",
            "Epoch [1/4], Step [3], Loss: 1415.0208\n",
            "Epoch [1/4], Step [4], Loss: 142.8376\n",
            "Epoch [1/4], Step [5], Loss: 15.0409\n",
            "Epoch [1/4], Step [6], Loss: 18.4935\n",
            "Epoch [1/4], Step [7], Loss: 1.5072\n",
            "Epoch [1/4], Step [8], Loss: 2.3866\n",
            "Epoch [1/4], Step [9], Loss: 1.9493\n",
            "Epoch [1/4], Step [10], Loss: 1.6929\n",
            "Epoch [1/4], Step [11], Loss: 7.4244\n",
            "Epoch [1/4], Step [12], Loss: 1.9380\n",
            "Epoch [1/4], Step [13], Loss: 1.9354\n",
            "Epoch [1/4], Step [14], Loss: 1.9284\n",
            "Epoch [1/4], Step [15], Loss: 1.9260\n",
            "Epoch [1/4], Step [16], Loss: 1.9303\n",
            "Epoch [1/4], Step [17], Loss: 2.0134\n",
            "Epoch [1/4], Step [18], Loss: 1.9096\n",
            "Epoch [1/4], Step [19], Loss: 1.8977\n",
            "Epoch [1/4], Step [20], Loss: 1.9029\n",
            "Epoch [1/4], Step [21], Loss: 1.8755\n",
            "Epoch [1/4], Step [22], Loss: 1.8738\n",
            "Epoch [1/4], Step [23], Loss: 1.8607\n",
            "Epoch [1/4], Step [24], Loss: 1.8615\n",
            "Epoch [1/4], Step [25], Loss: 1.9048\n",
            "Epoch [1/4], Step [26], Loss: 1.8573\n",
            "Epoch [1/4], Step [27], Loss: 1.8705\n",
            "Epoch [1/4], Step [28], Loss: 1.8759\n",
            "Epoch [1/4], Step [29], Loss: 1.8284\n",
            "Epoch [1/4], Step [30], Loss: 1.8674\n",
            "Epoch [1/4], Step [31], Loss: 1.8147\n",
            "Epoch [1/4], Step [32], Loss: 1.7820\n",
            "Epoch [1/4], Step [33], Loss: 1.8272\n",
            "Epoch [1/4], Step [34], Loss: 1.7928\n",
            "Epoch [1/4], Step [35], Loss: 1.8149\n",
            "Epoch [1/4], Step [36], Loss: 1.7770\n",
            "Epoch [1/4], Step [37], Loss: 1.8515\n",
            "Epoch [1/4], Step [38], Loss: 1.7783\n",
            "Epoch [1/4], Step [39], Loss: 1.7891\n",
            "Epoch [1/4], Step [40], Loss: 1.7824\n",
            "Epoch [1/4], Step [41], Loss: 1.6771\n",
            "Epoch [1/4], Step [42], Loss: 1.7070\n",
            "Epoch [1/4], Step [43], Loss: 1.8478\n",
            "Epoch [1/4], Step [44], Loss: nan\n",
            "Epoch [1/4], Step [45], Loss: nan\n",
            "Epoch [1/4], Step [46], Loss: nan\n",
            "Epoch [1/4], Step [47], Loss: nan\n",
            "Epoch [1/4], Step [48], Loss: nan\n",
            "Epoch [1/4], Step [49], Loss: nan\n",
            "Epoch [1/4], Step [50], Loss: nan\n",
            "Epoch [1/4], Step [51], Loss: nan\n",
            "Epoch [1/4], Step [52], Loss: nan\n",
            "Epoch [1/4], Step [53], Loss: nan\n",
            "Epoch [1/4], Step [54], Loss: nan\n",
            "Epoch [1/4], Step [55], Loss: nan\n",
            "Epoch [1/4], Step [56], Loss: nan\n",
            "Epoch [1/4], Step [57], Loss: nan\n",
            "Epoch [1/4], Step [58], Loss: nan\n",
            "Epoch [1/4], Step [59], Loss: nan\n",
            "Epoch [1/4], Step [60], Loss: nan\n",
            "Epoch [1/4], Step [61], Loss: nan\n",
            "Epoch [1/4], Step [62], Loss: nan\n",
            "Epoch [1/4], Step [63], Loss: nan\n",
            "Epoch [1/4], Step [64], Loss: nan\n",
            "Epoch [1/4], Step [65], Loss: nan\n",
            "Epoch [1/4], Step [66], Loss: nan\n",
            "Epoch [1/4], Step [67], Loss: nan\n",
            "Epoch [1/4], Step [68], Loss: nan\n",
            "Epoch [1/4], Step [69], Loss: nan\n",
            "Epoch [1/4], Step [70], Loss: nan\n",
            "Epoch [1/4], Step [71], Loss: nan\n",
            "Epoch [1/4], Step [72], Loss: nan\n",
            "Epoch [1/4], Step [73], Loss: nan\n",
            "Epoch [1/4], Step [74], Loss: nan\n",
            "Epoch [1/4], Step [75], Loss: nan\n",
            "Epoch [1/4], Step [76], Loss: nan\n",
            "Epoch [1/4], Step [77], Loss: nan\n",
            "Epoch [1/4], Step [78], Loss: nan\n",
            "Epoch [1/4], Step [79], Loss: nan\n",
            "Epoch [1/4], Step [80], Loss: nan\n",
            "Epoch [1/4], Step [81], Loss: nan\n",
            "Epoch [1/4], Step [82], Loss: nan\n",
            "Epoch [1/4], Step [83], Loss: nan\n",
            "Epoch [1/4], Step [84], Loss: nan\n",
            "Epoch [1/4], Step [85], Loss: nan\n",
            "Epoch [1/4], Step [86], Loss: nan\n",
            "Epoch [1/4], Step [87], Loss: nan\n",
            "Epoch [1/4], Step [88], Loss: nan\n",
            "Epoch [1/4], Step [89], Loss: nan\n",
            "Epoch [1/4], Step [90], Loss: nan\n",
            "Epoch [1/4], Step [91], Loss: nan\n",
            "Epoch [1/4], Step [92], Loss: nan\n",
            "Epoch [1/4], Step [93], Loss: nan\n",
            "Epoch [1/4], Step [94], Loss: nan\n",
            "Epoch [1/4], Step [95], Loss: nan\n",
            "Epoch [1/4], Step [96], Loss: nan\n",
            "Epoch [1/4], Step [97], Loss: nan\n",
            "Epoch [1/4], Step [98], Loss: nan\n",
            "Epoch [1/4], Step [99], Loss: nan\n",
            "Epoch [1/4], Step [100], Loss: nan\n",
            "Epoch [1/4], Step [101], Loss: nan\n",
            "Epoch [1/4], Step [102], Loss: nan\n",
            "Epoch [1/4], Step [103], Loss: nan\n",
            "Epoch [1/4], Step [104], Loss: nan\n",
            "Epoch [1/4], Step [105], Loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-072d94f7ecf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size = b\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0,10000,batch_size):\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = labels_new[i:i + batch_size] \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #if (i+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{int(i/batch_size + 1)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-5H9EHDFOao",
        "outputId": "2ec2aa66-1b52-45dd-c0c8-218fd72a3758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50 test images: 72.0 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i in range(100,200, batch_size):\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = labels_new[i:i + batch_size] \n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 50 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QXNgilIrVe",
        "outputId": "6578bf96-fa4b-41cf-e20b-062c76207c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50 test images: 74.0 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i in range(200,300, batch_size):\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = labels_new[i:i + batch_size] \n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 50 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i in range(600,700, batch_size):\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = labels_new[i:i + batch_size] \n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 50 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEf5nmEHyQfD",
        "outputId": "04f8be5b-0023-4883-b7c7-4b2539907ebd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50 test images: 68.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i in range(0,845, batch_size):\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = labels_new[i:i + batch_size] \n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 50 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM6yFx2K05lP",
        "outputId": "dbda47d0-04e7-403d-e339-6cbb847b5f8b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50 test images: 65.44378698224853 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh1g9XJckZOn2n7HzpyzW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}