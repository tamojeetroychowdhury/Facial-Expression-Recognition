{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjSSS0KQVl9W/V80a+ciT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamojeetroychowdhury/Facial-Expression-Recognition/blob/main/Week-2/FER_Neural_Net_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqmuwRGrBA9B",
        "outputId": "d6b2af8b-2647-4c7b-ff2c-b17cd43aead6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Data**\n",
        "I used the FER-2013 dataset, downloaded from Kaggle at [this link](https://www.kaggle.com/datasets/deadskull7/fer2013?resource=download). It has 7 different emotions and over 35,000 images. The dataset was available in a nice CSV format directly."
      ],
      "metadata": {
        "id": "1M0XN2J0CUXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UXckMDJsA2U8"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('gdrive/My Drive/fer2013.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The emotion of disgust corresponding to label '1' had very few samples (about 500) which would be difficult to work with, so that row is dropped."
      ],
      "metadata": {
        "id": "LiUZBj5GC5Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df.emotion == 1].index)"
      ],
      "metadata": {
        "id": "jMjFNkfjXNVa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = np.array(df['emotion'][:35000])\n",
        "all_labels[np.where(all_labels == 6)] = 1   \n",
        "all_labels = torch.tensor(all_labels)\n",
        "#labels_wo_1 = all_labels[np.where(all_labels != 1)]"
      ],
      "metadata": {
        "id": "JZS-Y46mBd9k"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7K542ivHAdP",
        "outputId": "f8e7a9be-c8af-4160-8803-3ff409148831"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35000"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two cells below just convert the pixel data from string format to a 48 $\\times$ 48 matrix format, and creates a train of those in a single colour channel (greyscale), for a total of 35000 images. Hence the dimensions of 35000 $\\times$ 1 $\\times$ 48 $\\times$ 48."
      ],
      "metadata": {
        "id": "frvQeYNBDW7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = []\n",
        "for s in df['pixels'][:35000]:\n",
        "    m = s.split()\n",
        "    l = np.zeros((48,48))\n",
        "    for i in range(len(m)):\n",
        "        n = int(m[i])\n",
        "        l[i//48][i%48] = n\n",
        "    ds.append([l])"
      ],
      "metadata": {
        "id": "tZCphk5JBlT1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = np.array(ds, dtype = np.float32)\n",
        "dataset = torch.tensor(ds)\n",
        "#dataset.view(10000,1,48,48)\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaa61v-cBo6s",
        "outputId": "efd8b63b-ee7c-4578-f8ff-ba84d57fdb5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35000, 1, 48, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xTeKMiXECXu6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convolutional Neural network**\n",
        "The one implemented below is a very basic one with two convolutional layers and two linear layers.  \n",
        "I played around with the number of output channels in the convolutional layers slighly (very large numbers ate up lots of RAM) and 20 and 30 seemed to work best."
      ],
      "metadata": {
        "id": "IX2V0COjECSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 2304\n",
        "hidden_size = 500 \n",
        "num_classes = 10\n",
        "num_epochs = 3\n",
        "batch_size = 2000\n",
        "learning_rate = 0.001\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(20, 30, 5)\n",
        "        self.fc1 = nn.Linear(30 * 9 * 9, 90)\n",
        "        self.fc3 = nn.Linear(90, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) \n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 30 * 9 * 9)            \n",
        "        x = F.relu(self.fc1(x))                             \n",
        "        x = self.fc3(x)                       \n",
        "        return x"
      ],
      "metadata": {
        "id": "w97XQMIcC4j3"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "GGjg07HIEEmL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore num_epochs, I ran the cell repeatedly with different values of num_epochs and checked for accuracy after each run. The training loop ran for a total of 40 times.  \n",
        "Accuracy on test dataset increased from 11\\% (which is worse than a random classifier!) to 45.7\\% (still very bad, but I hope to be able to increase that with tweaks in the neural network)."
      ],
      "metadata": {
        "id": "wL2bv68cEbvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5000\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0,30000,batch_size):\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = all_labels[i:i + batch_size] \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #if (i+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{int(i/batch_size + 1)}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "Imi_a9jMEgb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb959267-23ec-4cf2-945c-72195e4da044"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [1], Loss: 1.0757\n",
            "Epoch [1/4], Step [2], Loss: 1.0265\n",
            "Epoch [1/4], Step [3], Loss: 1.0685\n",
            "Epoch [1/4], Step [4], Loss: 1.0394\n",
            "Epoch [1/4], Step [5], Loss: 1.0217\n",
            "Epoch [1/4], Step [6], Loss: 1.0150\n",
            "Epoch [2/4], Step [1], Loss: 1.0652\n",
            "Epoch [2/4], Step [2], Loss: 1.0299\n",
            "Epoch [2/4], Step [3], Loss: 1.0576\n",
            "Epoch [2/4], Step [4], Loss: 1.0353\n",
            "Epoch [2/4], Step [5], Loss: 1.0062\n",
            "Epoch [2/4], Step [6], Loss: 1.0135\n",
            "Epoch [3/4], Step [1], Loss: 1.0633\n",
            "Epoch [3/4], Step [2], Loss: 1.0153\n",
            "Epoch [3/4], Step [3], Loss: 1.0562\n",
            "Epoch [3/4], Step [4], Loss: 1.0331\n",
            "Epoch [3/4], Step [5], Loss: 1.0273\n",
            "Epoch [3/4], Step [6], Loss: 0.9879\n",
            "Epoch [4/4], Step [1], Loss: 1.0377\n",
            "Epoch [4/4], Step [2], Loss: 1.0176\n",
            "Epoch [4/4], Step [3], Loss: 1.0499\n",
            "Epoch [4/4], Step [4], Loss: 1.0167\n",
            "Epoch [4/4], Step [5], Loss: 0.9961\n",
            "Epoch [4/4], Step [6], Loss: 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i in range(30000, 35000, batch_size):\n",
        "        #images = images.reshape(-1, 28*28).to(device)\n",
        "        images = dataset[i:i + batch_size] \n",
        "        labels = all_labels[i:i + batch_size] \n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 5000 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPR3-AbWGSxS",
        "outputId": "f94e0388-f5e1-48ea-afb3-6c37a9bde3c7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 5000 test images: 45.72 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total epochs done = 40  \n",
        "Final accuracy = 45.72\\%"
      ],
      "metadata": {
        "id": "rFdlX6gXy8jU"
      }
    }
  ]
}